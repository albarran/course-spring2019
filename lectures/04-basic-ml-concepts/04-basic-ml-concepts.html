<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Machine Learning</title>
    <meta charset="utf-8">
    <meta name="author" content="Itamar Caspi" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="style\middlebury.css" type="text/css" />
    <link rel="stylesheet" href="style\middlebury-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to Machine Learning
## for Economists
### Itamar Caspi
### December 30, 2018 (updated: 2019-02-07)

---




# How "Big" is Big Data?

&lt;midd-blockquote&gt;"_A billion years ago modern homo sapiens emerged. A billion minutes ago, Christianity began. A billion seconds ago, the IBM PC was released.
A billion Google searches ago ... was this morning._"  
.right[Hal Varian (2013)]&lt;/midd-blockquote&gt;

The 4 Vs of big data:  

+ Volume - Scale of data.  
+ Velocity - Analysis of streaming data.  
+ Variety - Different forms of data.  
+ Veracity - Uncertainty of data.  


---

# From Big Data to "Data Science"

&lt;img src="figures/venn.jpg" width="75%" style="display: block; margin: auto;" /&gt;


---

# But Then Again...

&lt;img src="figures/frame.jpg" width="60%" style="display: block; margin: auto;" /&gt;


---
# Outline

1. Basic concepts  

--

2. The problem of overfitting  

--

3. Too complext? Regularize!  

--

4. What's in it for economists?


---
class: inverse, center, middle
# Basic concepts


---
# So, What is Machine Learning (ML)?

A modern definition of __machine learning__ from computer science (Mitchel, 1997):

&lt;midd-blockquote&gt;
"A computer program is said to __learn__ from experience `\(E\)` with respect to some class of tasks `\(T\)` and performance measure `\(P\)`, if its performance at tasks in `\(T\)`, as measured by `\(P\)`, improves with experience `\(E\)`."
&lt;/midd-blockquote&gt;

Example: Self driving car  
`\(E=\)` data from sensors, camera, GPS, etc.  
`\(T=\)` driving.  
`\(P=\)` errors per mile.  

---
# Machine Learning is a Subfield of Artificial Intelligence (AI)


&lt;img src="figures/DL-ML-AI.png" width="70%" style="display: block; margin: auto;" /&gt;

Source: [https://www.magnetic.com/blog/explaining-ai-machine-learning-vs-deep-learning-post/](https://www.magnetic.com/blog/explaining-ai-machine-learning-vs-deep-learning-post/)


---
# Types of ML problems

There are three broad classifications of ML problems:
  + supervised learning (prediction).
  + unsupervised learning (knowledge discovery).
  + reinforcement learning (e.g., AlphaGo. Not covered in this lecture).
  
---
# Unsupervised Learning

In unsupervised learning, our goal is to divide high-dimensional data into groups of observations that are __similar__ in their features `\((X)\)`.

Examples of algorithms:  
  + Principal component analysis (dimensionality reduction)  
  + Cluster Analysis  
  
Applications:  
  + Face recognition
  + Topic analysis


---
# Example 1: Clustering OECD Inflation Rates

&lt;img src="figures/clustInflationCropped.jpg" width="80%" style="display: block; margin: auto;" /&gt;

.footnote[_Source_: Baudot-Trajtenberg and Caspi (2018).]
---
# Example 2: Topic Modeling of Bank Of Israel Minutes


&lt;img src="figures/LDA.png" width="70%" style="display: block; margin: auto;" /&gt;


---
# Supervised Learning

Consider a stable data generating process:

`$$Y=f(\boldsymbol{X})+\varepsilon$$`
where `\(Y\)` is the outcome variable, `\(\boldsymbol{X}\)` is a vector of predictiors ("features"), and `\(\varepsilon\)` is the irreducable error.  



---
# The Bias-Variance Decomposition

Under a squared error loss function, an optimal predictive model is one that minimizes the __expected__ squared prediction error.  

It can be shown that if the true model is `\(Y=f(X)+\epsilon\)`, then

`$$\begin{aligned}[t]
\mathbb{E}\left[\text{SE}^0\right] &amp;= \mathbb{E}\left[(y^0 - \hat{f}(x^0))^2\right] \\ &amp;= \underbrace{\left(\mathbb{E}(\hat{f}(x^0)) - f(x^0)\right)^{2}}_{\text{bias}^2} + \underbrace{\mathbb{E}\left[\hat{f}(x^0) - \mathbb{E}(\hat{f}(x^0))\right]^2}_{\text{variance}} + \underbrace{\mathbb{E}\left[y^0 - f(x^0)\right]^{2}}_{\text{irreducible error}} \\ &amp;= \underbrace{\mathrm{Bias}^2 + \mathbb{V}[\hat{f}(x^0)]}_{\text{reducible error}} + \sigma^2_{\epsilon}
\end{aligned}$$`


---
# The Variance of f(X)


---
# The Bias of f(X)





---
# Illustrating the Bias-Variance Tradeoff


```r
library(tidyverse)

set.seed(1203) # for replicating the simulation

df &lt;- crossing(economist = c("A", "B", "C"),
         obs = 1:10) %&gt;% 
  mutate(economist = as.factor(economist)) %&gt;% 
  mutate(income = rnorm(n(), mean = 100, sd = 10)) %&gt;% 
  mutate(consumption = 10 + 0.5 * income + rnorm(n(), sd = 10))

knitr::kable(head(df), format = "html")
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; economist &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; obs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; income &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; consumption &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; A &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 113.75832 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 51.97065 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; A &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 100.34093 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 41.57210 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; A &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 91.88489 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 67.83440 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; A &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 94.87317 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 60.25032 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; A &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 115.24757 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 79.45907 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; A &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 91.12396 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 51.34953 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
# Scatterplot of the Data

.pull-left[

```r
df %&gt;% 
  ggplot(aes(x = consumption,
             y = income)) +
  geom_point()
```
]
.pull-right[
![](04-basic-ml-concepts_files/figure-html/unnamed-chunk-7-1..svg)&lt;!-- --&gt;
]
---
# Split the Sample Between 3 Economists

.pull-left[

```r
df %&gt;% 
  ggplot(aes(x = consumption,
             y = income,
             color = economist)) +
  geom_point() +
  theme(legend.position = "bottom")
```
]
.pull-right[
![](04-basic-ml-concepts_files/figure-html/unnamed-chunk-8-1..svg)&lt;!-- --&gt;
]

---
# High Bias, Low Variance

`$$Y_i = \beta_0+\varepsilon_i$$`
.pull-left[

```r
df %&gt;% 
  ggplot(aes(x = consumption,
             y = income,
             color = economist)) +
  geom_point() +
  geom_smooth(method = lm,
*             formula = y ~ 1,
              se = FALSE,
              color = "black") +
  facet_wrap(~ economist) +
  geom_vline(xintercept = 70, linetype = "dashed") +
  theme(legend.position = "bottom")
```
]
.pull-right[
![](04-basic-ml-concepts_files/figure-html/unnamed-chunk-9-1..svg)&lt;!-- --&gt;
]
---
# Low Bias, High Variance

`$$Y_i = \beta_0+\beta_1X_i+\beta_2X^2_i+\beta_3X_i^3+\beta_4X_i^4+\beta_5X_i^5+\varepsilon_i$$`
.pull-left[

```r
df %&gt;% 
  ggplot(aes(x = consumption,
             y = income,
             color = economist)) +
  geom_point() +
  geom_smooth(method = lm,
*             formula = y ~ poly(x,5),
              se = FALSE,
              color = "black") +
  facet_wrap(~ economist) +
  geom_vline(xintercept = 70, linetype = "dashed") +
  theme(legend.position = "bottom")
```
]
.pull-right[
![](04-basic-ml-concepts_files/figure-html/unnamed-chunk-10-1..svg)&lt;!-- --&gt;
]

---
# Bias and Variance Just Right"

`$$Y_i = \beta_0+\beta_1 X_i + \varepsilon_i$$`
.pull-left[

```r
df %&gt;% 
  ggplot(aes(x = consumption,
             y = income,
             color = economist)) +
  geom_point() +
  geom_smooth(method = lm,
*             formula = y ~ x,
              se = FALSE,
              color = "black") +
  facet_wrap(~ economist) +
  geom_vline(xintercept = 70, linetype = "dashed") +
  theme(legend.position = "bottom")
```
]
.pull-right[
![](04-basic-ml-concepts_files/figure-html/unnamed-chunk-11-1..svg)&lt;!-- --&gt;
]
---
# The Typical Bias-Variance Tradeoff in ML


Typically, prediction machines strive to find levels of bias and variance that are "just right":

&lt;img src="figures/biasvariance.png" width="80%" style="display: block; margin: auto;" /&gt;


---
# When is the Bias-Variance Tradeoff Important?

In low-dimenssional settings ( `\(n\gg p\)` )  
  + overfitting is highly unlikely  
  + training MSE closely approximates test MSE  
  + conventional tools (e.g., OLS) will perform well on a test set

INTUITION: As `\(n\rightarrow\infty\)`, insignificant terms will converge to their true value (zero).

In high-dimenssional settings ( `\(n\ll p\)` )  
  + overfitting is highly unlikely  
  + training MSE poorly approximates test MSE  
  + conventional tools tend to overfit  
  
&lt;blockquote&gt; `\(n\ll p\)` is prevalent in big-data &lt;/blockquote&gt;
  
  
  
---
# The Bias-Variance Tradeoff in Low-dmenssional Settings

The model:  
`$$Y_i = \beta_0+\beta_1X_i+\beta_2X^2_i+\beta_3X_i^3+\varepsilon_i$$`
where the sample size for each economist is now `\(N=100\)`.

.pull-left[

```r
set.seed(1505)
crossing(economist = c("A", "B", "C"),
         obs = 1:500) %&gt;% 
  mutate(economist = as.factor(economist)) %&gt;% 
  mutate(income = rnorm(n(), mean = 100, sd = 10)) %&gt;% 
  mutate(consumption = 10 + 0.5 * income + rnorm(n(), sd = 10)) %&gt;% 
  ggplot(aes(x = consumption,
             y = income,
             color = economist)) +
  geom_point() +
  geom_smooth(method = lm,
*             formula = y ~ poly(x,3),
              se = FALSE,
              color = "black") +
  facet_wrap(~ economist) +
  geom_vline(xintercept = 70, linetype = "dashed") +
  theme(legend.position = "bottom")
```
]
.pull-right[
![](04-basic-ml-concepts_files/figure-html/unnamed-chunk-13-1..svg)&lt;!-- --&gt;
]

---
# More Intuition on the Bias Variance Trade Off

Imagine you are a teaching assistant grading exams. You grade the first exam. What the prediction of the next grade should be?

+ the first test score is an unbiased estimator of the mean grade  
+ but it is extremely variable  
+ Solution: Think like a Bayesian and shrink towards your prior for the mean grade  


---
# Let's Simulate This

We now generate 100 datapoints on grades that comefrom the truncated normal distribution

`$$grade_i \sim truncN(a = 0, b = 100, \mu = 75, \sigma = 15),\quad i=1,2$$`


and the calculate two types of predictions - __unbiased__ estimate (previous grade) and __shrinked__ is a simple average of the previous grade and our prior mean grade (60)



Here a small sample from our simulated table:
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; attempt &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; grade1 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; grade2 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; unbiased_pred &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; shrinked_pred &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 193 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 51 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 60 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 51 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 60.5 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 168 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 89 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 82 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 89 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 79.5 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 744 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 78 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 75.0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 937 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 82 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 75 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 82 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 76.0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 225 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 82 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 82 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 76.0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 840 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 51 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 84 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 51 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 60.5 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
 
---
# The Distribution of Prediction and the RMSE

&lt;img src="04-basic-ml-concepts_files/figure-html/unnamed-chunk-16-1..svg" style="display: block; margin: auto;" /&gt;

And now for the RMSE  
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; unbiased_RMSE &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; shrinked_RMSE &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 311.792 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 199.1132 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
The shrinked prediction turns out to be better then the unbiased one!

---

class: title-slide-section-blue, center, middle

# Too Complex? Regularize!



---

# How Can We Detect Overfitting?

Given that the test-MSE is __unobservable__, how can we know whether our model overfits the training set?  

+ Main idea in machine learning - use __less__ data!  
+ Key point: Fit the model on a subset of the training set, and predict the subset that was __not__ used to fit the model.  
+ When will this "magic" work? Recall our "stable environment" assumption.  


---
# TBA

&lt;div align="center"&gt;
&lt;img src="figures/train_validate.pdf"&gt;
&lt;/div&gt;

---

# Validation

&lt;img src="figures/train_validate.png" width="50%" style="display: block; margin: auto;" /&gt;



1. Devide the sample set into three parts: a training set, a validation set and a test set.

2. Fit the model to the training set.

3. Use the estimated model to predict outcomes from the validation set.

4. Use the mean of the squared prediction errors from the validation set to approximate the text-MSE


---
# K-fold Cross-Validation (CV)


---

# Example: Tuning the Optimal Polynimial Degree


.pull-left[
Assume that the "true" data-generating process is a 4th degree polynimial (i.e., `\(\lambda=4\)`):

`$$Y_i = 1 + 2X_i - 4X_i^2 + 1.5X_i^3 - 0.5X_i^4 + \varepsilon_i$$`

Let's draw `\(n=200\)` samples of `\(x\)` and `\(y\)`

```r
set.seed(1111) # for replicating the results

n  &lt;- 200
df &lt;- tibble(x     = rnorm(n),
             y     = 1 + 2*x - 4*x^2 + 1.5*x^3 - 0.5*x^4 + 6*rnorm(n))

df %&gt;% 
  ggplot(aes(x, y)) +
  geom_point()
```
]
.pull-right[
![](04-basic-ml-concepts_files/figure-html/unnamed-chunk-19-1..svg)&lt;!-- --&gt;
]
 
 
---
# Step 1: Train-test Split

We first need to load the __tidymodels__ package

```r
library(tidymodels)
```

We will use the `initial_split()`, `training()` and `testing()` functions from the `rsample` package to perform an initial train-test split


```r
df_split &lt;- initial_split(df, prop = 0.75)

df_split
```

```
## &lt;150/50/200&gt;
```



```r
training_df &lt;- training(df_split) # the trainig set
testting_df &lt;- testing(df_split)  # the test set

training_df
```

```
## # A tibble: 150 x 2
##          x          y
##      &lt;dbl&gt;      &lt;dbl&gt;
##  1 -0.0866   -0.00804
##  2  1.17     -6.59   
##  3  0.116    -7.16   
##  4 -2.93   -112.     
##  5  0.678     4.28   
##  6  1.12     -1.01   
##  7  1.28     -4.55   
##  8 -0.976     1.99   
##  9  0.991     1.95   
## 10 -1.53    -31.6    
## # ... with 140 more rows
```

---
# Prepaer Folds for Cross-validation


```r
cv_data &lt;- training_df %&gt;% 
  vfold_cv(v = 5) %&gt;%  
  mutate(train     = map(splits, ~training(.x)), 
         validate  = map(splits, ~testing(.x)))

cv_data
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 4
##   splits           id    train              validate         
## * &lt;list&gt;           &lt;chr&gt; &lt;list&gt;             &lt;list&gt;           
## 1 &lt;split [120/30]&gt; Fold1 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;
## 2 &lt;split [120/30]&gt; Fold2 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;
## 3 &lt;split [120/30]&gt; Fold3 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;
## 4 &lt;split [120/30]&gt; Fold4 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;
## 5 &lt;split [120/30]&gt; Fold5 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;
```

---
# Set Search Range for Lambda


```r
cv_tune &lt;- cv_data %&gt;% 
  crossing(lambda = 1:6)

cv_tune
```

```
## # A tibble: 30 x 5
##    splits           id    train              validate          lambda
##    &lt;list&gt;           &lt;chr&gt; &lt;list&gt;             &lt;list&gt;             &lt;int&gt;
##  1 &lt;split [120/30]&gt; Fold1 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      1
##  2 &lt;split [120/30]&gt; Fold1 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      2
##  3 &lt;split [120/30]&gt; Fold1 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      3
##  4 &lt;split [120/30]&gt; Fold1 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      4
##  5 &lt;split [120/30]&gt; Fold1 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      5
##  6 &lt;split [120/30]&gt; Fold1 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      6
##  7 &lt;split [120/30]&gt; Fold2 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      1
##  8 &lt;split [120/30]&gt; Fold2 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      2
##  9 &lt;split [120/30]&gt; Fold2 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      3
## 10 &lt;split [120/30]&gt; Fold2 &lt;tibble [120 x 2]&gt; &lt;tibble [30 x 2]&gt;      4
## # ... with 20 more rows
```

---
# Estimating CV-MSE


```r
cv_mse &lt;- cv_tune %&gt;% 
  mutate(model = map2(lambda, train, ~ lm(y ~ poly(x, .x), data = .y))) %&gt;% 
  mutate(predicted = map2(model, validate, ~ augment(.x, newdata = .y))) %&gt;% 
  unnest(predicted) %&gt;% 
  group_by(lambda) %&gt;% 
  summarise(mse = mean((.fitted - y)^2))

cv_mse
```

```
## # A tibble: 6 x 2
##   lambda   mse
##    &lt;int&gt; &lt;dbl&gt;
## 1      1 160. 
## 2      2  53.1
## 3      3  35.8
## 4      4  28.7
## 5      5  31.8
## 6      6  77.0
```

---
# Finding the Optimal Lambda
 
&lt;img src="04-basic-ml-concepts_files/figure-html/unnamed-chunk-26-1..svg" style="display: block; margin: auto;" /&gt;


---
# ML vs. Econometrics

Apart from jargon (Training set vs. in-sample, test-set vs. out of sample, learn vs. estimate, etc.) here is a summary of some of the key differences between ML and econometrics:


|   Machine Learning | Econometrics |
| -----------------: | ----------------------: |
| predict | explain |
| `\(\hat{Y}\)` | `\(\hat{\beta}\)` |
| minimize test MSE | unbiasedness |
| --- | inference |
| stable environment | counterfactual analysis |
| black-box | structural |
| high-dimensional | low-dimensional |


---
# Prediction in Aid of Estimation

`$$Y_i=\alpha+\underbrace{\tau T_i}_{\text{low dimensional}} +\underbrace{\sum_{j=1}^{p}\beta_{j}X_{ij}}_{\text{high dimensional}}+\varepsilon_i,\quad\text{for }i=1,\dots,n$$`
where
+ An outcome `\(Y_i\)`  
+ A treatment assignment `\(T_i\in\{0,1\}\)`  
+ A vector of `\(p\)` controls `\(X_i\)`  

Our object of interest is often `\(\hat{\tau}\)`, the average treatment effect (ATE).
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"slideNumberFormat": "<div class=\"progress-bar-container\">   <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">   </div> </div> "
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
